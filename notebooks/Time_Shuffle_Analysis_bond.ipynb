{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%reload_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_key = \"bon\", 3, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal, day, epoch = epoch_key\n",
    "data_type, dim = \"clusterless\", \"1D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load file: /data2/edeno/replay_trajectory_paper/src/../Raw-Data/Bond/bonDIO03.mat\n",
      "No DIO file found, using distance from well to segment trials\n",
      "Failed to load file: /data2/edeno/replay_trajectory_paper/src/../Raw-Data/Bond/bonDIO03.mat\n",
      "No DIO file found, inferring correct inbound/outbound from task rules\n",
      "Failed to load file: /data2/edeno/replay_trajectory_paper/src/../Raw-Data/Bond/bonDIO03.mat\n",
      "No DIO file found, using distance from well to segment trials\n",
      "Failed to load file: /data2/edeno/replay_trajectory_paper/src/../Raw-Data/Bond/bonDIO03.mat\n",
      "No DIO file found, inferring correct inbound/outbound from task rules\n"
     ]
    }
   ],
   "source": [
    "from src.load_data import load_data\n",
    "\n",
    "data = load_data(epoch_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loren_frank_data_processing.position import make_track_graph\n",
    "from loren_frank_data_processing.position import EDGE_ORDER, EDGE_SPACING\n",
    "from src.parameters import ANIMALS\n",
    "\n",
    "is_training = data[\"position_info\"].speed > 4\n",
    "position = data[\"position_info\"].loc[:, \"linear_position\"]\n",
    "track_labels = data[\"position_info\"].arm_name\n",
    "track_graph, center_well_id = make_track_graph(epoch_key, ANIMALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "\n",
    "def spike_time_shuffle(multiunit):\n",
    "    \"\"\"Shuffle each tetrode spike time by a random amount\n",
    "    \"\"\"\n",
    "    n_time = len(multiunit.time)\n",
    "    n_tetrodes = len(multiunit.tetrodes)\n",
    "\n",
    "    rand_time_offset = np.random.randint(\n",
    "        low=-(n_time - 1) // 2, high=(n_time - 1) // 2, size=n_tetrodes\n",
    "    )\n",
    "    shuffled_multiunit = [\n",
    "        multiunit.isel(tetrodes=tetrode_ind, drop=False).roll(\n",
    "            time=time_offset_ind, roll_coords=False\n",
    "        )\n",
    "        for tetrode_ind, time_offset_ind in enumerate(rand_time_offset)\n",
    "    ]\n",
    "\n",
    "    return xr.concat(shuffled_multiunit, dim=multiunit.tetrodes).transpose(\n",
    "        *multiunit.dims\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "from signal import SIGUSR1, SIGUSR2, signal\n",
    "from subprocess import PIPE, run\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from dask.distributed import Client\n",
    "from loren_frank_data_processing import save_xarray\n",
    "from loren_frank_data_processing.position import make_track_graph\n",
    "from replay_trajectory_classification import (\n",
    "    ClusterlessClassifier,\n",
    "    SortedSpikesClassifier,\n",
    ")\n",
    "from scipy.ndimage import label\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis import (\n",
    "    get_linear_position_order,\n",
    "    get_place_field_max,\n",
    "    get_replay_info,\n",
    "    reshape_to_segments,\n",
    ")\n",
    "from src.load_data import load_data\n",
    "from src.parameters import (\n",
    "    ANIMALS,\n",
    "    FIGURE_DIR,\n",
    "    PROBABILITY_THRESHOLD,\n",
    "    PROCESSED_DATA_DIR,\n",
    "    SAMPLING_FREQUENCY,\n",
    "    TRANSITION_TO_CATEGORY,\n",
    "    continuous_transition_types,\n",
    "    discrete_diag,\n",
    "    knot_spacing,\n",
    "    model,\n",
    "    model_kwargs,\n",
    "    movement_var,\n",
    "    place_bin_size,\n",
    "    replay_speed,\n",
    "    spike_model_penalty,\n",
    ")\n",
    "from src.visualization import (\n",
    "    plot_category_counts,\n",
    "    plot_category_duration,\n",
    "    plot_neuron_place_field_2D_1D_position,\n",
    "    plot_ripple_decode_1D,\n",
    "    plot_ripple_decode_2D,\n",
    ")\n",
    "\n",
    "FORMAT = \"%(asctime)s %(message)s\"\n",
    "\n",
    "logging.basicConfig(level=\"INFO\", format=FORMAT, datefmt=\"%d-%b-%y %H:%M:%S\")\n",
    "\n",
    "\n",
    "def classify(\n",
    "    position,\n",
    "    classifier,\n",
    "    data,\n",
    "    is_training,\n",
    "    track_graph,\n",
    "    center_well_id,\n",
    "    edge_order,\n",
    "    edge_spacing,\n",
    "    epoch_key,\n",
    "    name=\"actual\",\n",
    "):\n",
    "    animal, day, epoch = epoch_key\n",
    "    data_type, dim = \"clusterless\", \"1D\"\n",
    "\n",
    "    classifier.fit(\n",
    "        position,\n",
    "        data[\"multiunit\"],\n",
    "        is_training=is_training,\n",
    "        track_graph=track_graph,\n",
    "        center_well_id=center_well_id,\n",
    "        edge_order=edge_order,\n",
    "        edge_spacing=edge_spacing,\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    is_test = ~is_training\n",
    "\n",
    "    test_groups = pd.DataFrame(\n",
    "        {\"test_groups\": label(is_test.values)[0]}, index=is_test.index\n",
    "    )\n",
    "    immobility_results = []\n",
    "    for _, df in tqdm(\n",
    "        test_groups.loc[is_test].groupby(\"test_groups\"), desc=\"immobility\"\n",
    "    ):\n",
    "        start_time, end_time = df.iloc[0].name, df.iloc[-1].name\n",
    "        test_multiunit = data[\"multiunit\"].sel(time=slice(start_time, end_time))\n",
    "        test_multiunit = spike_time_shuffle(test_multiunit)\n",
    "        immobility_results.append(\n",
    "            classifier.predict(test_multiunit, time=test_multiunit.time)\n",
    "        )\n",
    "\n",
    "    immobility_results = xr.concat(immobility_results, dim=\"time\")\n",
    "\n",
    "    results = [\n",
    "        (\n",
    "            immobility_results.sel(\n",
    "                time=slice(df.start_time, df.end_time)\n",
    "            ).assign_coords(time=lambda ds: ds.time - ds.time[0])\n",
    "        )\n",
    "        for _, df in data[\"ripple_times\"].iterrows()\n",
    "    ]\n",
    "\n",
    "    results = xr.concat(results, dim=data[\"ripple_times\"].index).assign_coords(\n",
    "        state=lambda ds: ds.state.to_index().map(TRANSITION_TO_CATEGORY)\n",
    "    )\n",
    "\n",
    "    logging.info(\"Saving results...\")\n",
    "    save_xarray(\n",
    "        os.path.join(PROCESSED_DATA_DIR, \"time_shuffle\"),\n",
    "        epoch_key,\n",
    "        results.drop([\"causal_posterior\"]),\n",
    "        group=f\"/{data_type}/{dim}/classifier/time_shuffle/{name}\",\n",
    "    )\n",
    "\n",
    "    logging.info(\"Calculating replay_info...\")\n",
    "    ripple_times = data[\"ripple_times\"].loc[:, [\"start_time\", \"end_time\"]]\n",
    "    spikes = (\n",
    "        ((data[\"multiunit\"].sum(\"features\") > 0) * 1.0)\n",
    "        .to_dataframe(name=\"spikes\")\n",
    "        .unstack()\n",
    "    )\n",
    "    spikes.columns = data[\"tetrode_info\"].tetrode_id\n",
    "    ripple_spikes = reshape_to_segments(spikes, ripple_times)\n",
    "    track_graph, _ = make_track_graph(epoch_key, ANIMALS)\n",
    "    replay_info = get_replay_info(\n",
    "        results,\n",
    "        spikes,\n",
    "        data[\"ripple_times\"],\n",
    "        data[\"position_info\"],\n",
    "        track_graph,\n",
    "        SAMPLING_FREQUENCY,\n",
    "        PROBABILITY_THRESHOLD,\n",
    "        epoch_key,\n",
    "        classifier,\n",
    "        data[\"ripple_consensus_trace_zscore\"],\n",
    "    )\n",
    "\n",
    "    logging.info(\"Saving replay_info...\")\n",
    "    epoch_identifier = f\"{animal}_{day:02d}_{epoch:02d}_{data_type}_{dim}\"\n",
    "    replay_info_filename = os.path.join(\n",
    "        PROCESSED_DATA_DIR, \"time_shuffle\", f\"{epoch_identifier}_replay_info_{name}.csv\"\n",
    "    )\n",
    "    replay_info.to_csv(replay_info_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClusterlessClassifier(discrete_transition_diag=0.98, movement_var=6.0,\n",
       "                      occupancy_kwargs={'bandwidth': array([24., 24., 24., 24.,  6.,  6.])},\n",
       "                      occupancy_model=<class 'replay_trajectory_classification.misc.NumbaKDE'>,\n",
       "                      place_bin_size=3.0, replay_speed=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ClusterlessClassifier(\n",
    "    place_bin_size=place_bin_size,\n",
    "    movement_var=movement_var,\n",
    "    replay_speed=replay_speed,\n",
    "    discrete_transition_diag=discrete_diag,\n",
    "    continuous_transition_types=continuous_transition_types,\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03-Jan-21 07:06:40 Fitting initial conditions...\n",
      "03-Jan-21 07:06:41 Fitting state transition...\n",
      "03-Jan-21 07:06:41 Fitting multiunits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f69f8e37cc4546b669a9d60b5401a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='immobility'), FloatProgress(value=0.0, max=359.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for shuffle_ind in range(50):\n",
    "    classify(\n",
    "        position,\n",
    "        classifier,\n",
    "        data,\n",
    "        is_training,\n",
    "        track_graph,\n",
    "        center_well_id,\n",
    "        EDGE_ORDER,\n",
    "        EDGE_SPACING,\n",
    "        epoch_key,\n",
    "        name=f\"time_shuffle_{shuffle_ind:02d}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:replay_trajectory_paper] *",
   "language": "python",
   "name": "conda-env-replay_trajectory_paper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
